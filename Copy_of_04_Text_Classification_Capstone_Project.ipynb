{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nowknowing/text-classification/blob/main/Copy_of_04_Text_Classification_Capstone_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://weclouddata.s3.amazonaws.com/images/logos/wcd_logo_new_2.png\"  width='25%'>  \n",
        "\n",
        "\n",
        "Developed by WeCloudData\n",
        "<br></br>"
      ],
      "metadata": {
        "id": "BeY3EN42K_A7"
      },
      "id": "BeY3EN42K_A7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRtNPGruI01r"
      },
      "source": [
        "# Capstone Project: Your First NLP Classification Model\n",
        "\n",
        "Welcome to your first capstone project in natural language processing (NLP)! In this notebook, you will build a complete text classification pipeline using Python and the SMS Spam Collection dataset. You'll learn how to:\n",
        "\n",
        "- Load and explore the dataset\n",
        "- Clean and preprocess text data\n",
        "- Convert text into numerical features using TF-IDF\n",
        "- Split the data into training and testing sets\n",
        "- Adjust hyperparameters and train several classification models\n",
        "- Evaluate model performance\n",
        "\n",
        "After following along with the SMS spam example, you'll be encouraged to try similar techniques on other text datasets.\n",
        "\n",
        "Let's get started!"
      ],
      "id": "JRtNPGruI01r"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym8rDA51I01x"
      },
      "source": [
        "## Step 1: Choosing a Dataset\n",
        "\n",
        "For this project, we are using the **SMS Spam Collection** dataset. You can download the dataset from the following link:\n",
        "\n",
        "[Kaggle SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset)\n",
        "\n",
        "Make sure to download the CSV file (commonly named `spam.csv`) and place it in the same folder as this notebook."
      ],
      "id": "Ym8rDA51I01x"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3InzVov1I01z"
      },
      "outputs": [],
      "source": [
        "# Example: Loading the SMS Spam Collection dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (ensure spam.csv is in your working directory)\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "# The dataset may contain extra unnamed columns; keep only the relevant ones\n",
        "df = df[['v1', 'v2']]\n",
        "df.columns = ['label', 'message']\n",
        "\n",
        "print('Example Dataset: SMS Spam Collection')\n",
        "display(df.head())"
      ],
      "id": "3InzVov1I01z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ht5OWedBI01y"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "1. Download the SMS Spam Collection dataset from Kaggle.\n",
        "2. Alternatively, choose another text dataset of your interest.\n",
        "3. Place the dataset file in the working directory and note the file name for later use."
      ],
      "id": "ht5OWedBI01y"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEzqRId7I010"
      },
      "source": [
        "## Step 2: Exploring the Data\n",
        "\n",
        "In this step, we will explore the SMS Spam Collection dataset by:\n",
        "\n",
        "- Viewing the first few rows\n",
        "- Getting summary statistics using `.describe()`\n",
        "- Checking the data structure with `.info()`\n",
        "- Looking for missing values\n",
        "\n",
        "This initial exploration helps you understand the dataset before diving into text preprocessing."
      ],
      "id": "hEzqRId7I010"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RAIp_duI012"
      },
      "outputs": [],
      "source": [
        "# View the first few rows\n",
        "print('First 5 rows of the SMS Spam dataset:')\n",
        "display(df.head())\n",
        "\n",
        "# Summary statistics\n",
        "print('Summary statistics:')\n",
        "display(df.describe())\n",
        "\n",
        "# Data structure and missing values\n",
        "print('Dataset info:')\n",
        "display(df.info())\n",
        "\n",
        "print('Missing values in each column:')\n",
        "display(df.isnull().sum())"
      ],
      "id": "9RAIp_duI012"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Missing values in each column:')\n",
        "display(df.isnull().sum())"
      ],
      "metadata": {
        "id": "WNzMXModsT-M"
      },
      "id": "WNzMXModsT-M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BqPDbznhI011"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "Using the dataset you loaded (or your own dataset):\n",
        "\n",
        "1. Display the first few rows of the dataset.\n",
        "2. Print summary statistics and the dataset information.\n",
        "3. Identify any missing values in the dataset."
      ],
      "id": "BqPDbznhI011"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHQaE3fwI012"
      },
      "source": [
        "## Step 3: Text Cleaning and Preprocessing\n",
        "\n",
        "Real-world text data is often noisy. In this step, you'll clean and preprocess your text data by:\n",
        "\n",
        "- Converting text to lowercase\n",
        "- Removing punctuation and special characters\n",
        "- Removing extra whitespace\n",
        "\n",
        "This will help standardize the data for effective feature extraction."
      ],
      "id": "HHQaE3fwI012"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uWi5ntcI014"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and non-alphabetic characters\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "# Apply text cleaning to the 'message' column\n",
        "df['clean_message'] = df['message'].apply(clean_text)\n",
        "\n",
        "print('Cleaned text sample:')\n",
        "display(df[['message', 'clean_message']].head())"
      ],
      "id": "6uWi5ntcI014"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-W8317twI013"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "1. Write a function to clean the text data using the steps described above.\n",
        "2. Apply your cleaning function to the text column of your dataset.\n",
        "3. Inspect the cleaned text to ensure it meets expectations."
      ],
      "id": "-W8317twI013"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c88rUxVgI015"
      },
      "source": [
        "## Step 4: Feature Engineering - Converting Text to Numerical Features\n",
        "\n",
        "Machine learning models require numerical input. One common approach to convert text data into numbers is TF-IDF vectorization. In this step, we will:\n",
        "\n",
        "- Use `TfidfVectorizer` from Scikit-Learn to convert text into numerical TF-IDF features"
      ],
      "id": "c88rUxVgI015"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RpJK8b0LI016"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "\n",
        "# Fit and transform the cleaned text\n",
        "X_features = vectorizer.fit_transform(df['clean_message'])\n",
        "\n",
        "print('TF-IDF features shape:', X_features.shape)"
      ],
      "id": "RpJK8b0LI016"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep0JBB__I015"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "1. Use `TfidfVectorizer` to transform your cleaned text data into TF-IDF features.\n",
        "2. (Optional) Experiment with other vectorizers such as `CountVectorizer` and compare the results."
      ],
      "id": "Ep0JBB__I015"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKvvPtZWI016"
      },
      "source": [
        "## Step 5: Splitting the Data\n",
        "\n",
        "Before training our text classification models, we need to split the dataset into training and testing sets. The target variable here is `label`, indicating whether a message is spam or not."
      ],
      "id": "mKvvPtZWI016"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXI4s4ddI017"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define features (X) and target (y)\n",
        "X = X_features\n",
        "y = df['label']\n",
        "\n",
        "# Split the data into 80% training and 20% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print('Training set shape:', X_train.shape)\n",
        "print('Testing set shape:', X_test.shape)"
      ],
      "id": "gXI4s4ddI017"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zRT3QRcI016"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "1. Define your features (`X`) and target (`y`).\n",
        "2. Use `train_test_split` to divide your data into training and test sets (e.g., 80% training, 20% testing)."
      ],
      "id": "9zRT3QRcI016"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_qK7TPSI017"
      },
      "source": [
        "## Step 5.5: Adjusting Hyperparameters\n",
        "\n",
        "Before training the models, it's important to set and adjust hyperparameters. For text classification, we will use:\n",
        "\n",
        "- **Logistic Regression:** Adjust `max_iter`\n",
        "- **Multinomial Naive Bayes:** Adjust `alpha`\n",
        "- **Support Vector Machine (SVM):** Adjust `C` and kernel parameters"
      ],
      "id": "W_qK7TPSI017"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B91mrbfgI018"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Initialize models with hyperparameters\n",
        "lr_model = LogisticRegression(max_iter=200, solver='lbfgs')\n",
        "nb_model = MultinomialNB(alpha=1.0)\n",
        "svm_model = SVC(C=1.0, kernel='linear', probability=True)\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': lr_model,\n",
        "    'Multinomial Naive Bayes': nb_model,\n",
        "    'SVM': svm_model\n",
        "}\n",
        "\n",
        "print('Models initialized with adjusted hyperparameters:')\n",
        "for name, model in models.items():\n",
        "    print(name, model)"
      ],
      "id": "B91mrbfgI018"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8afMFvfI017"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "1. Initialize each model with the suggested hyperparameters.\n",
        "2. (Optional) Experiment with different hyperparameter values and observe how they affect performance."
      ],
      "id": "v8afMFvfI017"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AT8rSUUTI018"
      },
      "source": [
        "## Step 6: Training Different Classification Models\n",
        "\n",
        "Now it's time to train our text classification models using the training data from the SMS Spam dataset."
      ],
      "id": "AT8rSUUTI018"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBg2fqluI018"
      },
      "outputs": [],
      "source": [
        "# Train each model and store predictions\n",
        "predictions = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    pred = model.predict(X_test)\n",
        "    predictions[name] = pred\n",
        "    print(f\"{name} model trained.\")\n",
        "\n",
        "print('\\nAll models have been trained on the SMS Spam dataset!')"
      ],
      "id": "nBg2fqluI018"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qlS7tlHI018"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "1. Train each model on the training set and store the predictions for the test set.\n",
        "2. (Optional) Try adding an additional model or two to compare performance."
      ],
      "id": "4qlS7tlHI018"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SM66cAPjI018"
      },
      "source": [
        "## Step 7: Evaluating Model Performance\n",
        "\n",
        "After training our models, it's essential to evaluate their performance on the test data. We will use:\n",
        "\n",
        "- A classification report to assess metrics such as accuracy, precision, recall, and F1-score\n",
        "- A confusion matrix to visualize the distribution of correct and incorrect predictions"
      ],
      "id": "SM66cAPjI018"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWS1m7F3I019"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "for name, pred in predictions.items():\n",
        "    print(f\"\\nModel: {name}\")\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_test, pred))\n",
        "    print('Confusion Matrix:')\n",
        "    print(confusion_matrix(y_test, pred))\n",
        "    print('---------------------------')"
      ],
      "id": "gWS1m7F3I019"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuMffmYNI019"
      },
      "source": [
        "### Exercise\n",
        "\n",
        "1. For each model, generate a classification report and confusion matrix.\n",
        "2. Compare the performance metrics to determine which model performs best on the SMS Spam dataset.\n",
        "3. Write a brief analysis of why one model might outperform the others based on the metrics."
      ],
      "id": "BuMffmYNI019"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgylRgmuI019"
      },
      "source": [
        "## Final Thoughts and Next Steps\n",
        "\n",
        "Great job! You have now built a complete NLP classification pipeline using the SMS Spam Collection dataset:\n",
        "\n",
        "1. **Choosing a Dataset:** Downloaded the SMS Spam Collection dataset from Kaggle.\n",
        "2. **Loading and Exploring the Data:** Loaded and explored the dataset.\n",
        "3. **Text Cleaning and Preprocessing:** Cleaned and preprocessed the text data.\n",
        "4. **Feature Engineering:** Converted text into numerical features using TF-IDF.\n",
        "5. **Splitting the Data:** Divided the data into training and testing sets.\n",
        "6. **Adjusting Hyperparameters:** Set and adjusted hyperparameters for each model.\n",
        "7. **Training Models:** Built several text classification models.\n",
        "8. **Evaluating Performance:** Assessed model performance on unseen data.\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Experiment with other text datasets and replicate these steps.\n",
        "- Explore additional text preprocessing techniques (e.g., stemming, lemmatization, or advanced stopword removal).\n",
        "- Try more advanced models, including deep learning approaches for NLP.\n",
        "\n",
        "Keep exploring and enjoy your journey into natural language processing!"
      ],
      "id": "ZgylRgmuI019"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZGrqo5bI019"
      },
      "source": [
        "## References and Further Reading\n",
        "\n",
        "Here are some useful resources for the modules and functions used in this notebook:\n",
        "\n",
        "- **Pandas:** [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
        "- **Scikit-Learn:** [Scikit-Learn Documentation](https://scikit-learn.org/stable/documentation.html)\n",
        "  - [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
        "  - [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
        "  - [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
        "  - [MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
        "  - [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)\n",
        "- **Regular Expressions (re):** [Python re Module](https://docs.python.org/3/library/re.html)\n",
        "- **NLTK:** [NLTK Documentation](https://www.nltk.org/)\n",
        "\n",
        "These resources will help you dive deeper into the topics covered in this notebook."
      ],
      "id": "rZGrqo5bI019"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}